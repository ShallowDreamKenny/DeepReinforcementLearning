{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(1000):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间 = Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "动作空间 = Discrete(2)\n",
      "动作数 = 2\n",
      "初始状态 = None\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))\n",
    "print('动作数 = {}'.format(env.action_space.n))\n",
    "print('初始状态 = {}'.format(env.state))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m observation, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset(seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10000000\u001B[39m):\n\u001B[1;32m----> 4\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n\u001B[0;32m      7\u001B[0m         observation, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \n\u001B[0;32m     42\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m \n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 50\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:187\u001B[0m, in \u001B[0;36mCartPoleEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    184\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 187\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:299\u001B[0m, in \u001B[0;36mCartPoleEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    297\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mevent\u001B[38;5;241m.\u001B[39mpump()\n\u001B[0;32m    298\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclock\u001B[38;5;241m.\u001B[39mtick(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrender_fps\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m--> 299\u001B[0m     \u001B[43mpygame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mtranspose(\n\u001B[0;32m    303\u001B[0m         np\u001B[38;5;241m.\u001B[39marray(pygame\u001B[38;5;241m.\u001B[39msurfarray\u001B[38;5;241m.\u001B[39mpixels3d(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscreen)), axes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    304\u001B[0m     )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(10000000):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始状态 = (array([-0.00912169, -0.02823715,  0.00883062, -0.01829591], dtype=float32), {})\n",
      "初始状态 = [-0.00912169 -0.02823715  0.00883062 -0.01829591]\n"
     ]
    }
   ],
   "source": [
    "init_state = env.reset()\n",
    "print('初始状态 = {}'.format(init_state))\n",
    "print('初始状态 = {}'.format(env.state))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "动作 = 0: 当前状态 = [ 0.05470503 -0.02067758 -0.10901434 -0.18609284], 奖励 = 1.0, 结束标志 = False, ??=False, 日志信息 = {}\n",
      "动作 = 1: 当前状态 = [ 0.05429149  0.17582157 -0.1127362  -0.51107866], 奖励 = 1.0, 结束标志 = False, ??=False, 日志信息 = {}\n",
      "动作 = 1: 当前状态 = [ 0.05780791  0.372336   -0.12295777 -0.83705336], 奖励 = 1.0, 结束标志 = False, ??=False, 日志信息 = {}\n",
      "动作 = 0: 当前状态 = [ 0.06525464  0.1790886  -0.13969883 -0.5854293 ], 奖励 = 1.0, 结束标志 = False, ??=False, 日志信息 = {}\n",
      "动作 = 1: 当前状态 = [ 0.06883641  0.37586236 -0.15140742 -0.9186525 ], 奖励 = 1.0, 结束标志 = False, ??=False, 日志信息 = {}\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _, info = env.step(action)\n",
    "    print('动作 = {0}: 当前状态 = {1}, 奖励 = {2}, 结束标志 = {3}, ??={4}, 日志信息 = {5}'.format(action, state, reward, done,_, info))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = [-0.03529537 -0.1478901  -0.02141205  0.24508767]; reward = 1.0\n",
      "state = [-0.03825318 -0.3426998  -0.0165103   0.53094053]; reward = 1.0\n",
      "state = [-0.04510717 -0.14734954 -0.00589149  0.2331013 ]; reward = 1.0\n",
      "state = [-0.04805417 -0.3423868  -0.00122946  0.52392006]; reward = 1.0\n",
      "state = [-0.0549019  -0.14724758  0.00924894  0.23084997]; reward = 1.0\n",
      "state = [-0.05784685 -0.34250048  0.01386594  0.5264359 ]; reward = 1.0\n",
      "state = [-0.06469686 -0.14757633  0.02439466  0.23815429]; reward = 1.0\n",
      "state = [-0.06764839  0.04718877  0.02915774 -0.04673513]; reward = 1.0\n",
      "state = [-0.06670462  0.24188074  0.02822304 -0.3300778 ]; reward = 1.0\n",
      "state = [-0.061867    0.4365898   0.02162149 -0.6137286 ]; reward = 1.0\n",
      "state = [-0.0531352   0.631403    0.00934691 -0.899524  ]; reward = 1.0\n",
      "state = [-0.04050714  0.43615568 -0.00864356 -0.6039177 ]; reward = 1.0\n",
      "state = [-0.03178403  0.6313974  -0.02072192 -0.89931065]; reward = 1.0\n",
      "state = [-0.01915608  0.826794   -0.03870813 -1.1984344 ]; reward = 1.0\n",
      "state = [-0.0026202   1.0223949  -0.06267682 -1.5029932 ]; reward = 1.0\n",
      "state = [ 0.0178277   1.218219   -0.09273668 -1.8145677 ]; reward = 1.0\n",
      "state = [ 0.04219208  1.0242437  -0.12902804 -1.5520805 ]; reward = 1.0\n",
      "state = [ 0.06267696  0.8308836  -0.16006964 -1.3022803 ]; reward = 1.0\n",
      "state = [ 0.07929463  1.0276327  -0.18611525 -1.640491  ]; reward = 1.0\n",
      "state = [ 0.09984729  0.83511525 -0.21892507 -1.4111043 ]; reward = 1.0\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import time\n",
    "# 生成环境\n",
    "env = gym.make('CartPole-v1',render_mode=\"human\")\n",
    "# 环境初始化\n",
    "state = env.reset()\n",
    "# 循环交互\n",
    "while True:\n",
    "    # 渲染画面\n",
    "    # env.render()\n",
    "    # 从动作空间随机获取一个动作\n",
    "    action = env.action_space.sample()\n",
    "    # agent与环境进行一步交互\n",
    "    state, reward, done, _, info = env.step(action)\n",
    "    print('state = {0}; reward = {1}'.format(state, reward))\n",
    "    # 判断当前episode 是否完成\n",
    "    if done:\n",
    "        print('done')\n",
    "        break\n",
    "    time.sleep(1)\n",
    "# 环境结束\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
