{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size,\n",
    "                 action_size,\n",
    "                 seed,\n",
    "                 buffer_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 tau=1e-3,\n",
    "                 lr=1e-3):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        self.BUFFER_SIZE = buffer_size  # 经验池大小\n",
    "        self.BATCH_SIZE = batch_size  # 训练用的批大小\n",
    "        self.GAMMA = gamma  # 折扣因子\n",
    "        self.TAU = tau  # 目标网络更新参数\n",
    "        self.LR = lr  # 优化器学习参数\n",
    "\n",
    "        # 神经网络\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.LR)\n",
    "\n",
    "        #经验池\n",
    "        self.buffer = ReplayBuffer(action_size, self.BUFFER_SIZE, self.BATCH_SIZE, seed)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def add_traj(self, traj):\n",
    "        \"\"\"将一条轨迹放入经验池中\"\"\"\n",
    "        for transition in traj:\n",
    "            self.buffer.memory.append(transition)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        \"\"\"\n",
    "        完成从状态到动作的映射\n",
    "        :param state:\n",
    "        :param eps: （float）贪婪策略的探索率\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.buffer) < 10 * self.BATCH_SIZE:\n",
    "            return 0\n",
    "        experiences = self.buffer.sample()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        Q_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        y = rewards + (self.GAMMA * Q_next * (1 - dones))\n",
    "        # states = torch.tensor(states.astype(int, copy=False),dtype=torch.int64,device=device)\n",
    "        Q = self.qnetwork_local(states).gather(1, actions)\n",
    "        loss = F.mse_loss(Q, y)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.TAU)\n",
    "\n",
    "    def soft_update(self, local_models, target_model, tau):\n",
    "        \"\"\"\n",
    "        两个神经网络参数之间的软更新\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        :param local_models:\n",
    "        :param target_model:\n",
    "        :param tau:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_models.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"用于存储智能体与环境交互的经验\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): 动作数量\n",
    "            buffer_size (int): 经验池大小\n",
    "            batch_size (int): 训练\n",
    "            seed (int): 随机种子\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.transition = namedtuple(\"Transition\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"从经验池中随机采样一定数量的样本.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(\n",
    "            device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(\n",
    "            device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):  # 方便别的地方使用len()获取经验池汇中transition的数量\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    估计价值网络\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size_1=256, hidden_size_2=128):\n",
    "        \"\"\"\n",
    "        初始化价值网络\n",
    "        :param state_size:\n",
    "        :param action_size:\n",
    "        :param seed:\n",
    "        :param hidden_size_1:\n",
    "        :param hidden_size_2:\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size_1, hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size_2, action_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import  gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间的形状:  (8,)\n",
      "动作空间的动作数量：  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('状态空间的形状: ', env.observation_space.shape)\n",
    "print('动作空间的动作数量： ',env.action_space.n )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "false INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\builder\\\\windows\\\\pytorch\\\\c10/cuda/CUDAGraphsC10Utils.h\":74, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus473639808",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m agent \u001B[38;5;241m=\u001B[39m \u001B[43mAgent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m20e5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[16], line 21\u001B[0m, in \u001B[0;36mAgent.__init__\u001B[1;34m(self, state_size, action_size, seed, buffer_size, batch_size, gamma, tau, lr)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLR \u001B[38;5;241m=\u001B[39m lr  \u001B[38;5;66;03m# 优化器学习参数\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# 神经网络\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_local \u001B[38;5;241m=\u001B[39m \u001B[43mQNetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_target \u001B[38;5;241m=\u001B[39m QNetwork(state_size, action_size, seed)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_local\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLR)\n",
      "Cell \u001B[1;32mIn[18], line 15\u001B[0m, in \u001B[0;36mQNetwork.__init__\u001B[1;34m(self, state_size, action_size, seed, hidden_size_1, hidden_size_2)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m初始化价值网络\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124;03m:param state_size:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m:param hidden_size_2:\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28msuper\u001B[39m(QNetwork, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseed \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnet \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m     17\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(state_size, hidden_size_1),\n\u001B[0;32m     18\u001B[0m     nn\u001B[38;5;241m.\u001B[39mReLU(),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(hidden_size_2, action_size),\n\u001B[0;32m     22\u001B[0m )\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\torch\\random.py:40\u001B[0m, in \u001B[0;36mmanual_seed\u001B[1;34m(seed)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcuda\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39m_is_in_bad_fork():\n\u001B[1;32m---> 40\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m default_generator\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\torch\\cuda\\random.py:113\u001B[0m, in \u001B[0;36mmanual_seed_all\u001B[1;34m(seed)\u001B[0m\n\u001B[0;32m    110\u001B[0m         default_generator \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdefault_generators[i]\n\u001B[0;32m    111\u001B[0m         default_generator\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n\u001B[1;32m--> 113\u001B[0m \u001B[43m_lazy_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\torch\\cuda\\__init__.py:165\u001B[0m, in \u001B[0;36m_lazy_call\u001B[1;34m(callable, **kwargs)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_lazy_call\u001B[39m(callable, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 165\u001B[0m         \u001B[43mcallable\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    167\u001B[0m         \u001B[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001B[39;00m\n\u001B[0;32m    168\u001B[0m         \u001B[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001B[39;00m\n\u001B[0;32m    169\u001B[0m         \u001B[38;5;66;03m# else here if this ends up being important.\u001B[39;00m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mglobal\u001B[39;00m _lazy_seed_tracker\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\torch\\cuda\\random.py:111\u001B[0m, in \u001B[0;36mmanual_seed_all.<locals>.cb\u001B[1;34m()\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(device_count()):\n\u001B[0;32m    110\u001B[0m     default_generator \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdefault_generators[i]\n\u001B[1;32m--> 111\u001B[0m     \u001B[43mdefault_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: false INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\builder\\\\windows\\\\pytorch\\\\c10/cuda/CUDAGraphsC10Utils.h\":74, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus473639808"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=8, action_size=4, seed=0, buffer_size=int(20e5), batch_size=256)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_agent(n_episodes=5000, max_t=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "\n",
    "    for episode_i in range(1, n_episodes + 1):\n",
    "        traj = []\n",
    "        state = env.reset()\n",
    "        score=0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # env.render()\n",
    "            if episode_i % 100 == 0:\n",
    "                env.render()\n",
    "\n",
    "            transition = agent.buffer.transition(state, action, reward, next_state, done)\n",
    "            traj.append(transition)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        agent.add_traj(traj)\n",
    "\n",
    "        for _ in range(int(len(traj) / 4)):\n",
    "            agent.learn()\n",
    "\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "\n",
    "        eps = max(eps_end, eps_decay * eps)  # 探索率衰减\n",
    "\n",
    "        print('\\rEpisode {}\\t平均得分: {:.2f}'.format(episode_i, np.mean(scores_window)), end=\"\")\n",
    "        if episode_i % 100 == 0:\n",
    "            print('\\rEpisode {}\\t平均得分: {:.2f}'.format(episode_i, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')  # 保存模型\n",
    "        if np.mean(scores_window) >= 200.0 and len(scores_window) >= 100:\n",
    "            print('\\n任务已经成功完成！总共经过 {:d} 次任务的训练。!\\t最近100次任务的平均得分为: {:.2f}'.format(episode_i - 100,\n",
    "                                                                                 np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint1.pth')  # 保存模型\n",
    "            break\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27\t平均得分: -148.86"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_agent\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 28\u001B[0m, in \u001B[0;36mtrain_agent\u001B[1;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001B[0m\n\u001B[0;32m     25\u001B[0m agent\u001B[38;5;241m.\u001B[39madd_traj(traj)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(traj) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m4\u001B[39m)):\n\u001B[1;32m---> 28\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m scores_window\u001B[38;5;241m.\u001B[39mappend(score)\n\u001B[0;32m     31\u001B[0m scores\u001B[38;5;241m.\u001B[39mappend(score)\n",
      "Cell \u001B[1;32mIn[2], line 64\u001B[0m, in \u001B[0;36mAgent.learn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     61\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmse_loss(Q, y)\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 64\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoft_update(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_local, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnetwork_target, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mTAU)\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "scores = train_agent()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(8,)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "state = env.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.00616913,  1.4158409 , -0.62487525,  0.21867698,  0.0071552 ,\n        0.14154342,  0.        ,  0.        ], dtype=float32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
