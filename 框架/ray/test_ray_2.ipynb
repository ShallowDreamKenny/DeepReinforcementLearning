{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 15:50:46,707\tWARNING ppo.py:351 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1334.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=8360)\u001B[0m 2022-12-11 15:50:52,546\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-12-11 15:50:53,960\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-38.15536366158485\n",
      "Iter: 1; avg. reward=-34.17466245965103\n",
      "Iter: 2; avg. reward=-29.522275936229164\n",
      "Iter: 3; avg. reward=-26.525985885700933\n",
      "Iter: 4; avg. reward=-23.788092573965663\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "# Define your problem using python and openAI's gym API:\n",
    "class ParrotEnv(gym.Env):\n",
    "    \"\"\"Environment in which an agent must learn to repeat the seen observations.\n",
    "\n",
    "    Observations are float numbers indicating the to-be-repeated values,\n",
    "    e.g. -1.0, 5.1, or 3.2.\n",
    "\n",
    "    The action space is always the same as the observation space.\n",
    "\n",
    "    Rewards are r=-abs(observation - action), for all steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # Make the space (for actions and observations) configurable.\n",
    "        super(ParrotEnv, self).render(mode='human')\n",
    "        self.action_space = config.get(\n",
    "            \"parrot_shriek_range\", gym.spaces.Box(-1.0, 1.0, shape=(1, )))\n",
    "        # Since actions should repeat observations, their spaces must be the\n",
    "        # same.\n",
    "        self.observation_space = self.action_space\n",
    "        self.cur_obs = None\n",
    "        self.episode_len = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the episode and returns the initial observation of the new one.\n",
    "        \"\"\"\n",
    "        # Reset the episode len.\n",
    "        self.episode_len = 0\n",
    "        # Sample a random number from our observation space.\n",
    "        self.cur_obs = self.observation_space.sample()\n",
    "        # Return initial observation.\n",
    "        return self.cur_obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes a single step in the episode given `action`\n",
    "\n",
    "        Returns:\n",
    "            New observation, reward, done-flag, info-dict (empty).\n",
    "        \"\"\"\n",
    "        # Set `done` flag after 10 steps.\n",
    "        self.episode_len += 1\n",
    "        done = self.episode_len >= 10\n",
    "        # r = -abs(obs - action)\n",
    "        reward = -sum(abs(self.cur_obs - action))\n",
    "        # Set a new observation (random sample).\n",
    "        self.cur_obs = self.observation_space.sample()\n",
    "        return self.cur_obs, reward, done, {}\n",
    "\n",
    "\n",
    "# Create an RLlib Algorithm instance from a PPOConfig to learn how to\n",
    "# act in the above environment.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        env=ParrotEnv,\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        env_config={\n",
    "            \"parrot_shriek_range\": gym.spaces.Box(-5.0, 5.0, (1, )),\n",
    "            \"render_mode\": 'human'\n",
    "        },\n",
    "    )\n",
    "    # Parallelize environment rollouts.\n",
    "    .rollouts(num_rollout_workers=3)\n",
    ")\n",
    "# Use the config's `build()` method to construct a PPO object.\n",
    "algo = config.build()\n",
    "\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "# Since we have to guess 10 times and the optimal reward is 0.0\n",
    "# (exact match between observation and action value),\n",
    "# we can expect to reach an optimal episode reward of 0.0.\n",
    "for i in range(5):\n",
    "    results = algo.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Apply the computed action in the environment.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m obs, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m---> 17\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Sum up rewards for reporting purposes.\u001B[39;00m\n\u001B[0;32m     19\u001B[0m total_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "File \u001B[1;32m~\\.conda\\envs\\RL\\lib\\site-packages\\gym\\core.py:147\u001B[0m, in \u001B[0;36mEnv.render\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;129m@abstractmethod\u001B[39m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;124;03m\"\"\"Renders the environment.\u001B[39;00m\n\u001B[0;32m    112\u001B[0m \n\u001B[0;32m    113\u001B[0m \u001B[38;5;124;03m    The set of supported modes varies per environment. (And some\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;124;03m                super(MyEnv, self).render(mode=mode) # just raise an exception\u001B[39;00m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Perform inference (action computations) based on given env observations.\n",
    "# Note that we are using a slightly simpler env here (-3.0 to 3.0, instead\n",
    "# of -5.0 to 5.0!), however, this should still work as the agent has\n",
    "# (hopefully) learned to \"just always repeat the observation!\".\n",
    "env = ParrotEnv({\"parrot_shriek_range\": gym.spaces.Box(-3.0, 3.0, (1, )),\"render_mode\":\"human\"})\n",
    "# Get the initial observation (some value between -10.0 and 10.0).\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "# Play one episode.\n",
    "while not done:\n",
    "    # Compute a single action, given the current observation\n",
    "    # from the environment.\n",
    "    action = algo.compute_single_action(obs)\n",
    "    # Apply the computed action in the environment.\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    # Sum up rewards for reporting purposes.\n",
    "    total_reward += reward\n",
    "# Report results.\n",
    "print(f\"Shreaked for 1 episode; total-reward={total_reward}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
