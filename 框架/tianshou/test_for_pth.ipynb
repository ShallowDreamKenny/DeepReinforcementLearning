{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tianshou\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 月球车模型测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "import tianshou as ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 建立月球车env环境"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "test_envs = ts.env.SubprocVectorEnv(\n",
    "        [lambda: gym.make('LunarLander-v2', render_mode=\"human\") for _ in range(1)]\n",
    "    )\n",
    "env = gym.make('LunarLander-v2')\n",
    "# env.render(mode='human')\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 建立深度网络模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "Q_param = {\"hidden_sizes\": [128, 128]}\n",
    "V_param = {\"hidden_sizes\": [128, 128]}\n",
    "net = ts.utils.net.common.Net(\n",
    "        state_shape,\n",
    "        action_shape,\n",
    "        hidden_sizes=[128, 128],\n",
    "        device='cuda',\n",
    "        dueling_param=(Q_param, V_param)\n",
    "    ).to('cuda')\n",
    "optim = torch.optim.Adam(net.parameters(), lr=0.013)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    net,\n",
    "    optim,\n",
    "    discount_factor=0.99, # gamma\n",
    "    estimation_step=3, # n_step\n",
    "    target_update_freq=500\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 载入模型\n",
    "policy.load_state_dict(torch.load('./policy.pth'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 建立collector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from tianshou.data import Collector\n",
    "\n",
    "eval_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "eval_collector.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 初始化policy（设置为eval模式）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "DQNPolicy(\n  (model): Net(\n    (model): MLP(\n      (model): Sequential(\n        (0): Linear(in_features=8, out_features=128, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=128, out_features=128, bias=True)\n        (3): ReLU()\n      )\n    )\n    (Q): MLP(\n      (model): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=128, out_features=128, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=128, out_features=4, bias=True)\n      )\n    )\n    (V): MLP(\n      (model): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=128, out_features=128, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n  (model_old): Net(\n    (model): MLP(\n      (model): Sequential(\n        (0): Linear(in_features=8, out_features=128, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=128, out_features=128, bias=True)\n        (3): ReLU()\n      )\n    )\n    (Q): MLP(\n      (model): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=128, out_features=128, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=128, out_features=4, bias=True)\n      )\n    )\n    (V): MLP(\n      (model): Sequential(\n        (0): Linear(in_features=128, out_features=128, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=128, out_features=128, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=128, out_features=1, bias=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "policy.set_eps(0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 开始测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "result = eval_collector.collect(n_episode=10, render=0.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward: 195.45919479232924, length: 382.2\n"
     ]
    }
   ],
   "source": [
    "rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 测试Atari环境"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from atari.atari_network import DQN\n",
    "from atari.atari_wrapper import make_atari_env\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.policy import DQNPolicy\n",
    "from tianshou.policy.modelbased.icm import ICMPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger, WandbLogger\n",
    "from tianshou.utils.net.discrete import IntrinsicCuriosityModule"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 本环境需改了tianshou中make_atari_env，以确保能够可视化测试环境"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 通过make_atari_env来创建环境，该环境缩小了输入图像，否则回合神经网络不一致"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Code\\python.code\\DeepReinforcementLearning\\框架\\tianshou\\atari\\atari_wrapper.py:349: UserWarning: Recommend using envpool (pip install envpool) to run Atari games more efficiently.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env, train_envs, test_envs = make_atari_env(\n",
    "        task= \"PongNoFrameskip-v4\",\n",
    "        seed=0,\n",
    "        training_num=1,\n",
    "        test_num=10,\n",
    "        scale=0,\n",
    "        frame_stack=4,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 初始化DQN网络"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.ArgumentParser()\n",
    "args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "args.action_shape = env.action_space.shape or env.action_space.n\n",
    "net = DQN(*args.state_shape, args.action_shape, device='cuda').to('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 初始化policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "# define policy\n",
    "policy = DQNPolicy(\n",
    "    net,\n",
    "    optim,\n",
    "    0.99,\n",
    "    4,\n",
    "    target_update_freq=500\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "DQNPolicy(\n  (model): DQN(\n    (net): Sequential(\n      (0): Sequential(\n        (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n        (1): ReLU(inplace=True)\n        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n        (3): ReLU(inplace=True)\n        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n        (5): ReLU(inplace=True)\n        (6): Flatten(start_dim=1, end_dim=-1)\n      )\n      (1): Linear(in_features=3136, out_features=512, bias=True)\n      (2): ReLU(inplace=True)\n      (3): Linear(in_features=512, out_features=6, bias=True)\n    )\n  )\n  (model_old): DQN(\n    (net): Sequential(\n      (0): Sequential(\n        (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n        (1): ReLU(inplace=True)\n        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n        (3): ReLU(inplace=True)\n        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n        (5): ReLU(inplace=True)\n        (6): Flatten(start_dim=1, end_dim=-1)\n      )\n      (1): Linear(in_features=3136, out_features=512, bias=True)\n      (2): ReLU(inplace=True)\n      (3): Linear(in_features=512, out_features=6, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.load_state_dict(torch.load('./atari_policy.pth'))\n",
    "policy.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 建立测试环境"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "test_envs = make_atari_env(\n",
    "        task= \"PongNoFrameskip-v4\",\n",
    "        seed=0,\n",
    "        training_num=1,\n",
    "        test_num=1,\n",
    "        scale=0,\n",
    "        frame_stack=4,\n",
    "        show=True,\n",
    "    )[2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 建立collector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from tianshou.data import Collector\n",
    "\n",
    "eval_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "eval_collector.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "policy.set_eps(0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 开始预览"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "result = eval_collector.collect(n_episode=10, render=0.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward: 19.9, length: 1770.7\n"
     ]
    }
   ],
   "source": [
    "rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
